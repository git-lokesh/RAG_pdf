# Local PDF RAG Chat (Streamlit + Ollama)

This project lets you chat with the content of your PDF files using a fully local Retrieval-Augmented Generation (RAG) pipeline.
Everything runs on your machine using Ollama, FAISS, and Streamlit.

## Features:
- Extract text from uploaded PDF documents.
- Split text into overlapping chunks for better retrieval.
- Build a FAISS vector store using mxbai-embed-large embeddings.
- Ask questions and get answers generated by Llama 3.1 (8B) locally.
- Cached vector store generation for fast re-runs.
- Chat-style interface with conversation history.
- Fully offline â€” no API keys needed.

## Requirements:
### Install dependencies:
    pip install -r requirements.txt

### Install Ollama:
    https://ollama.com

### Pull required models:
    ollama pull llama3.1:8b
    ollama pull mxbai-embed-large

### How to Run:
    streamlit run app.py

## Notes:
- All processing is offline.
- No cloud services involved.
- Works offline once models are downloaded.